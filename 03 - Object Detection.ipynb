{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 物件偵測\r\n",
        "\r\n",
        "*物件偵測*是一種電腦視覺形式，可在其中訓練機器學習模型，以便在影像中分類物件的個別執行個體，並指示標記物件位置的*周框方塊*您可以將其視為從*影像分類* (其中模型將回答「這是什麼的影像？」這一問題) 到組建解決方案的進展，在解決方案中我們可以詢問模型「影像中有什麼物件，以及這些物件在何處？」。\r\n",
        "\r\n",
        "![傀儡程式識別水果](./images/object-detection.jpg)\r\n",
        "\r\n",
        "例如，雜貨店可能使用物件偵測模型來實作自動化結帳系統，該系統使用相機掃描輸送帶，無需將每件產品放置在傳動帶上就能識別特定商品，還能逐個掃描。\r\n",
        "\r\n",
        "Microsoft Azure 中的**自訂視覺**認知服務為建立和發佈自訂物件偵測模型提供雲端式解決方案。\r\n",
        "\r\n",
        "## 建立自訂視覺資源\r\n",
        "\r\n",
        "若要使用自訂視覺服務，您需要可以用來訓練模型的 Azure 資源，以及可以發佈以供應用程式使用的資源。您可以將同一個資源用於每項工作，或者您可以為每項工作使用不同的資源來另行配置成本，前提是兩種資源均建立在同一區域。一項 (或者兩項) 工作的資源可以是一般性**認知服務**資源，或特定的**自訂視覺**資源。請使用以下指示來建立新**自訂視覺**資源 (或者如有現有資源，亦可使用)。\r\n",
        "\r\n",
        "1. 在新的瀏覽器索引標籤中，透過 [https://portal.azure.com](https://portal.azure.com) 開啟 Azure 入口網站，並使用與您的 Azure 訂用帳戶關聯的 Microsoft 帳戶登入。\r\n",
        "2. 選取 **[&#65291; 建立資源]** 按鈕，搜尋*自訂視覺*，並建立包含以下設定的**自訂視覺**資源：\r\n",
        "    - **建立選項**：兩個\r\n",
        "    - **訂用帳戶**： *您的 Azure 訂用帳戶* \r\n",
        "    - **資源群組**： *選取或建立具有唯一名稱的資源群組* \r\n",
        "    - **名稱**： *輸入唯一名稱* \r\n",
        "    - **訓練位置**： *選擇任一可用區域* \r\n",
        "    - **訓練定價層**：F0\r\n",
        "    - **預測位置**： *與訓練位置一致* \r\n",
        "    - **預測定價層**：F0\r\n",
        "\r\n",
        "    > **備註**：若您的訂用帳戶中已經有 F0 自訂視覺服務，請為這一個選取 **[S0]**。\r\n",
        "\r\n",
        "3. 等待資源建立。\r\n",
        "\r\n",
        "## 建立自訂視覺專案\r\n",
        "\r\n",
        "要訓練物件偵測模型，您需要以訓練資源為基礎建立自訂視覺專案。為此，您將使用自訂視覺入口網站。\r\n",
        "\r\n",
        "1. 在新的瀏覽器索引標籤中，透過 [https://customvision.ai](https://customvision.ai) 開啟自訂視覺入口網站，並使用與您的 Azure 訂用帳戶關聯的 Microsoft 帳戶登入。\r\n",
        "2. 建立一個包含以下設定的新專案：\r\n",
        "    - **名稱**：雜貨店偵測\r\n",
        "    - **描述**：針對雜貨店的物件偵測。\r\n",
        "    - **資源**： *先前建立的自訂視覺資源* \r\n",
        "    - **專案類型**：物件偵測\r\n",
        "    - **網域**：普通\r\n",
        "3. 等待專案建立並在瀏覽器中開啟。\r\n",
        "\r\n",
        "## 新增並標記影像\r\n",
        "\r\n",
        "要訓練物件偵測模型，您需要上傳影像 (該影像包含需要模型識別的類別)，並標記它們以指示每個物件執行個體的周框方塊。\r\n",
        "\r\n",
        "1.從 https://aka.ms/fruit-objects 中下載並擷取訓練影像。已擷取的資料夾包含水果影像的集合物件。**注意：** 作為臨時解決辦法，如果您無法存取訓練影像，請前往 https://www.github.com，然後前往 https://aka.ms/fruit-objects。 \r\n",
        "2. 在自訂視覺入口網站 [https://customvision.ai](https://customvision.ai) 中，確保您正在處理物件偵測專案 _Grocery Detection_。然後選擇 **[新增影像]** 並上傳擷取的資料夾中的所有影像。\r\n",
        "\r\n",
        "![透過按一下 [新增影像] 上傳下載的影像。](./images/fruit-upload.jpg)\r\n",
        "\r\n",
        "3. 影像上傳完成後，選取第一個以開啟它。\r\n",
        "4. 將滑鼠游標暫留在影像中的任一物件上，直到顯示自動偵測的區域，如下面的影像所示。然後選取物件，如有必要調整區域大小以便將其環繞。\r\n",
        "\r\n",
        "![物件的預設區域](./images/object-region.jpg)\r\n",
        "\r\n",
        "或者，您可以簡單地在物件周圍拖曳以建立區域。\r\n",
        "\r\n",
        "5. 當區域環繞物件時，新增具有適當物件類型 (*蘋果*、*香蕉*或*柳橙*) 的新標籤，如下所示：\r\n",
        "\r\n",
        "![影像中一個已標記的物件](./images/object-tag.jpg)\r\n",
        "\r\n",
        "6. 選取並標記影像中的每個其他物件，調整區域大小並根據需要新增新標籤。\r\n",
        "\r\n",
        "![影像中兩個已標記的物件](./images/object-tags.jpg)\r\n",
        "\r\n",
        "7. 使用右側的**>**連結轉到下一個影像，並標記其物件。然後，只需繼續處理整個影像集合物件，標記每個蘋果、香蕉和柳橙即可。\r\n",
        "\r\n",
        "8. 完成對最後一個影像的標記後，請關閉 **[影像詳細資料]** 編輯器，然後在 **[訓練影像]** 頁面上的 **[標記]** 下，選取 **[已標記]** 以查看所有已標記的影像：\r\n",
        "\r\n",
        "![專案中已標記的影像](./images/tagged-images.jpg)\r\n",
        "\r\n",
        "## 訓練並測試模型\r\n",
        "\r\n",
        "現在您已經在專案中標記影像，可以開始訓練模型。\r\n",
        "\r\n",
        "1. 在自訂視覺專案中，按一下 **[訓練]** 以使用已標記的影像訓練物件偵測模型。選取 **[快速訓練]** 選項。\r\n",
        "2. 等待訓練完成 (這可能需要十分鐘左右)，*然後檢閱精確度*、*重新叫用*和*對應*效能計量，這些計量測量分類模型的預測正確性，各項計量應該都較高。\r\n",
        "3. 在頁面的右上方，按一下 **[快速測試]**，然後在 **[影像 URL]** 方塊中，輸入 `https://aka.ms/apple-orange` 並檢視所生成的預測。然後關閉 **[快速測試]** 視窗。\r\n",
        "\r\n",
        "## 發佈並使用物件偵測模型\r\n",
        "\r\n",
        "現在，您可以準備發佈已訓練的模型，還可以從用戶端應用程式中使用該模型。\r\n",
        "\r\n",
        "1. 在 **[效能]** 頁面的左上方，按一下 **[&#128504; 發佈]** 以發佈包含以下設定的已訓練模型：\r\n",
        "    - **模型名稱**：偵測生產\r\n",
        "    - **預測資源**： *您的自訂視覺**預測**資源*。\r\n",
        "\r\n",
        "### (!)簽入 \r\n",
        "您是否使用了相同的模型名稱：**偵測生產**？ \r\n",
        "\r\n",
        "2. 發佈後，在 **[效能]** 頁面右上方按一下*設定* (&#9881;) 圖示，以檢視專案設定。然後，在 **[一般]** 下 (在左側)，複製**專案識別碼**。向下捲動並將其貼上到步驟 5 下面的程式碼儲存格中，取代 **YOUR_PROJECT_ID**。 \r\n",
        "\r\n",
        "> (如果您在本次練習開始時使用了**認知服務**資源，而不是建立**自訂視覺**資源，則可以從專案設定的右側複製其金鑰和端點，將其貼上到下面的程式碼儲存格中，然後執行該資源以查看結果。否則，請繼續完成以下步驟以獲取自訂視覺預測資源的金鑰和*端點*)。\r\n",
        "\r\n",
        "3. 在 **[專案設定]** 頁面的左上方，按一下*專案資源庫*(&#128065;) 圖示以退回到自訂視覺入口網站首頁，現在您的專案已在其中列出。\r\n",
        "\r\n",
        "4. 在自訂視覺入口網站首頁的右上方，按一下*設定*(&#9881;) 圖示以檢視自訂視覺服務的設定。然後，在**資源**下方，展開您的**預測**資源 (<u>而非</u>訓練資源) 並將其**金鑰**和**端點**值複製到步驟 5 下面的程式碼儲存格，取代 **YOUR_KEY** 和 **YOUR_ENDPOINT**。\r\n",
        "\r\n",
        "### (!)簽入 \r\n",
        "若您使用的是**自訂視覺**資源，您是否使用了**預測**資源 (<u>而非</u> 訓練資源)？\r\n",
        "\r\n",
        "5. 透過按一下 [執行儲存格] <span>&#9655;</span> 按鈕 (位於儲存格左上方) 執行下面的程式碼儲存格來為您的專案識別碼、金鑰和端點值設定變數。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "project_id = 'YOUR_PROJECT_ID' # Replace with your project ID\r\n",
        "cv_key = 'YOUR_KEY' # Replace with your prediction resource primary key\r\n",
        "cv_endpoint = 'YOUR_ENDPOINT' # Replace with your prediction resource endpoint\r\n",
        "\r\n",
        "model_name = 'detect-produce' # this must match the model name you set when publishing your model iteration exactly (including case)!\r\n",
        "print('Ready to predict using model {} in project {}'.format(model_name, project_id))"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692485387
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "現在，您可以將金鑰和端點與自訂視覺用戶端一起使用，以連線到您的自訂視覺物件偵測模型。\r\n",
        "\r\n",
        "執行以下程式碼儲存格，其使用您的模型來偵測影像中的個別產品。\r\n",
        "\r\n",
        "> **備註**：請勿過於擔心程式碼的詳細資料。其將 Python SDK 用於自訂視覺服務，以向您的模型提交影像並取出針對已偵測物件的預測。每個預測都由一個類別名稱 (蘋果**、香蕉**或柳橙**) 和周框方塊**座標組成，這些座標指示影像中已偵測到的預測物件之位置。然後，程式碼使用此資訊在影像上的每個物件周圍繪製一個帶標籤的方塊。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\r\n",
        "from msrest.authentication import ApiKeyCredentials\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from PIL import Image, ImageDraw, ImageFont\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# Load a test image and get its dimensions\r\n",
        "test_img_file = os.path.join('data', 'object-detection', 'produce.jpg')\r\n",
        "test_img = Image.open(test_img_file)\r\n",
        "test_img_h, test_img_w, test_img_ch = np.array(test_img).shape\r\n",
        "\r\n",
        "# Get a prediction client for the object detection model\r\n",
        "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\r\n",
        "predictor = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\r\n",
        "\r\n",
        "print('Detecting objects in {} using model {} in project {}...'.format(test_img_file, model_name, project_id))\r\n",
        "\r\n",
        "# Detect objects in the test image\r\n",
        "with open(test_img_file, mode=\"rb\") as test_data:\r\n",
        "    results = predictor.detect_image(project_id, model_name, test_data)\r\n",
        "\r\n",
        " Create a figure to display the results\r\n",
        "fig = plt.figure(figsize=(8, 8))\r\n",
        "plt.axis('off')\r\n",
        "\r\n",
        "# Display the image with boxes around each detected object\r\n",
        "draw = ImageDraw.Draw(test_img)\r\n",
        "lineWidth = int(np.array(test_img).shape[1]/100)\r\n",
        "object_colors = {\r\n",
        "    \"apple\": \"lightgreen\",\r\n",
        "    \"banana\": \"yellow\",\r\n",
        "    \"orange\": \"orange\"\r\n",
        "}\r\n",
        "for prediction in results.predictions:\r\n",
        "    color = 'white' # default for 'other' object tags\r\n",
        "    if (prediction.probability*100) > 50:\r\n",
        "        if prediction.tag_name in object_colors:\r\n",
        "            color = object_colors[prediction.tag_name]\r\n",
        "        left = prediction.bounding_box.left * test_img_w \r\n",
        "        top = prediction.bounding_box.top * test_img_h \r\n",
        "        height = prediction.bounding_box.height * test_img_h\r\n",
        "        width =  prediction.bounding_box.width * test_img_w\r\n",
        "        points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\r\n",
        "        draw.line(points, fill=color, width=lineWidth)\r\n",
        "        plt.annotate(prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100),(left,top), backgroundcolor=color)\r\n",
        "plt.imshow(test_img)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692585672
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "檢視結果預測，這些預測顯示偵測到的物件以及每個預測的可能性。"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}