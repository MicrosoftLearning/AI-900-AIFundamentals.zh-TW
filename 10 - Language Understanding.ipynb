{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 語言理解\r\n",
        "\r\n",
        "我們日益期望電腦能夠使用 AI 來理解以自然語言訴說語音或鍵入的命令。例如，您也許想實作首頁自動化系統，該系統能讓您在首頁中透過使用語音命令控制裝置，例如 「開燈」 或 「關燈」，並且有 AI 支援裝置可以理解命令並採取適當行動。\r\n",
        "\r\n",
        "![一個傀儡程式在聆聽](./images/language_understanding.jpg)\r\n",
        "\r\n",
        "## 建立製作和預測資源\r\n",
        "\r\n",
        "Microsoft 認知服務包括語言理解服務，該服務能讓您根據*表達*判斷應用於*實體*的*意圖*。 \r\n",
        "\r\n",
        "若要使用語言理解服務，您需要兩種資源：\r\n",
        "\r\n",
        "- *製作*資源：用於判斷、訓練以及測試語言模型。這必須是您的 Azure 訂用帳戶中的**語言理解 - 製作**資源。\r\n",
        "- *預測*資源：用於發佈模型並處理來自使用該資源的用戶端應用程式之要求。該資源可以是您的 Azure 訂用帳戶中的**語言理解**或**認知服務**資源。\r\n",
        "\r\n",
        "您可以使用**語言理解**或**認知服務**資源來*發佈*語言理解應用程式，但為了製作**應用程式，您必須建立獨立型**語言理解**資源。\r\n",
        "\r\n",
        "> **重點**：製作資源必須建立在三大*區域* (歐洲、澳大利亞或美國) 中的某一個區域。建立在歐洲或澳大利亞製作資源中的模型只能分別部署到位於歐洲或澳大利亞的預測資源；建立在美國製作資源中的模型可以部署到位於歐洲和澳大利亞之外的任何 Azure 位置的預測資源。有關比對製作和預測位置的詳細資料，請參閱 [製作和發佈區域文件](https://docs.microsoft.com/azure/cognitive-services/luis/luis-reference-regions)。\r\n",
        "\r\n",
        "1. 在其它瀏覽器索引標籤中，透過 [https://portal.azure.com](https://portal.azure.com) 開啟 Azure 入口網站，並用您的 Microsoft 帳戶登入。\r\n",
        "2. 按一下 **[+ 建立資源]**，並搜尋*語言理解*。\r\n",
        "3. 在服務清單中，按一下 **[語言理解]**。\r\n",
        "4. 在 **[語言理解]** 刀鋒視窗中，按一下 **[建立]**。\r\n",
        "5. 在 **[建立]** 刀鋒視窗中，輸入下列詳細資料並按一下 **[建立]**\r\n",
        "   - **建立選項**：兩個\r\n",
        "   - **名稱**： *您的服務之唯一名稱*\r\n",
        "   - **訂用帳戶**： *選取您的 Azure 訂用帳戶*\r\n",
        "   - **資源群組**： *選取現有的資源群組或建立新的資料群組*\r\n",
        "   - **製作位置**： *選取您慣用的位置*\r\n",
        "   - **製作定價層**：F0\r\n",
        "   - **預測位置**： *選取一個與您的製作位置在相同區域的位置*\r\n",
        "   - **預測定價層**：F0\r\n",
        "   \r\n",
        "6. 等待資源建立，並注意兩種已建置的兩種語言理解資源；一種是製作資源，另一種是預測資源。您可以透過導覽到建立它們的資源群組來檢視這些資源。\r\n",
        "\r\n",
        "### 建立語言理解應用程式\r\n",
        "\r\n",
        "若要使用語言理解實作自然語言理解，您可以建立一個應用程式；並新增實體、意圖和表達來判斷您想要此應用程式理解的命令：\r\n",
        "\r\n",
        "1. 在新的瀏覽器索引標籤中，開啟製作區域 (您已在其中建立製作資源) 的語言理解入口網站：\r\n",
        "    - 美國：[https://www.luis.ai](https://www.luis.ai)\r\n",
        "    - 歐洲：[https://eu.luis.ai](https://eu.luis.ai)\r\n",
        "    - 澳大利亞：[https://au.luis.ai](https://au.luis.ai)\r\n",
        "\r\n",
        "2. 請使用與您的 Azure 訂用帳戶關聯的 Microsoft 帳戶登入。若這是您第一次登入語言理解入口網站，您可能需要授與應用程式一些權限以存取您的詳細帳戶資料。然後透過選取剛才建立在您的 Azure 訂用帳戶中的現有語言理解製作資源來*完成歡迎*步驟。 \r\n",
        "\r\n",
        "3. 開啟 **[交談應用程式]** 頁面，並選取您的訂用帳戶和語言理解製作資源。然後建立一個適用於交談且包含以下設定全新應用程式：\r\n",
        "   - **名稱**：Home Automation\r\n",
        "   - **文化**： *中文 (台灣) (如果此選項不可用，讓它保持空白*)\r\n",
        "   - **描述**：簡單的首頁自動化\r\n",
        "   - **預測資源**： *您的語言理解預測資源*\r\n",
        "\r\n",
        "4. 若顯示帶有建立有效語言理解應用程式的提示窗格，請將其關閉。\r\n",
        "\r\n",
        "### 建立實體\r\n",
        "\r\n",
        "*實體*是您的語言模型可以識別並使用的事物。在此案例中，您的語言理解應用程式將用於控制 office 中的各種*裝置*，例如燈或風扇；所以您可以建立一個*裝置*實體，其中包括您想要應用程式使用的裝置類型清單。您可以為每一種裝置類型建立子清單，該清單識別裝置名稱 (例如 *light*) 和任何可能用於表示此裝置類型的同義字 (例如 *lamp*)。\r\n",
        "\r\n",
        "1. 在應用程式的語言理解頁面，左側窗格中，按一下 **[實體]**。然後按一下 **[建立]**，並建立名為**裝置**的新實體,選取 **[清單]** 類型，並按一下 **[建立]**。\r\n",
        "2. 在 **[清單項目]** 頁面中的標準**化值**下，鍵入 **[light]**，然後按 [ENTER]。\r\n",
        "3. 在新增 **light** 值之後，**在同義字**下鍵入 **[lamp]** 並按 [ENTER]。\r\n",
        "4. 新增第二個名為 **fan**的清單項目及其同義字 **AC**。\r\n",
        "\r\n",
        "> **備註**：在此實驗室中，依照指示使用準確的小寫或大寫文字 _(例如：light **而不是** Light)_，請勿新增額外的空格。 \r\n",
        "\r\n",
        "### 建立意圖\r\n",
        "\r\n",
        "*意圖*是您想要在一個或多個實體上執行的動作 - 例如，您可能想要開啟一盞燈或關閉風扇。在此案例中，您將判斷兩種意圖：一種是開啟裝置，另一種是關閉裝置。對於每一個意圖，您將指定樣本*表達*，此表達指示用於指示意圖的語言種類。\r\n",
        "\r\n",
        "> **備註**：對於此實驗室，請依照指示使用準確的小寫或大寫文字 _(例如：\"turn the light on”**而不是** \"Turn the light on \")_，請勿新增額外的空格。 \r\n",
        "\r\n",
        "1. 左側窗格中，按一下 **[意圖]**。然後按一下 **[建立]**，並新增名為 **switch_on** 的意圖，按一下 **[完成]**。\r\n",
        "2. 在**範例**標題和**範例使用者輸入**次標題下方，鍵入表達 ***[turn the light on]*** 並按 **[Enter]** 以將此表達提交至清單。\r\n",
        "3. 在表達 *turn the light on* 中，按一下文字 \"light”，並將其指派到**裝置**實體的 **light** 值。\r\n",
        "\r\n",
        "![如何將文字 \"light” 指派至實體值](./images/assign_entity.jpg)\r\n",
        "\r\n",
        "4. 新增第二個表達至 **switch_on** 意圖，以及片語 ***turn the fan on***。然後指派文字 \"fan” 至**裝置**實體的 **fan** 值。\r\n",
        "5. 在左側窗格中，按一下 **[意圖]** 和 **[建立]**，以新增第二個名為 **switch_off* *的意圖。\r\n",
        "6. 在 **switch_off** 意圖的 **[表達]** 頁面中，新增表達 ***turn the light off*** 並指派文字 \"light” 至**裝置**實體的 **light** 值。\r\n",
        "7. 新增第二個表達至 **switch_off** 意圖，以及片語 ***turn the fan off***。然後連線文字 \"fan” 至**裝置**實體的 **fan** 值。\r\n",
        "\r\n",
        "### 訓練和測試語言模型\r\n",
        "\r\n",
        "現在您可以準備使用在實體、意圖以及表達表單中已經提供的資料來為您的應用程式訓練語言模型。\r\n",
        "\r\n",
        "1. 在應用程式的 [語言理解] 頁面頂部，按一下 **[訓練]** 以訓練語言模型\r\n",
        "2. 模型完成訓練後，按一下 **[測試]**，並使用測試窗格來檢視適用於以下片語的預測意圖：\r\n",
        "    * *switch the light on*\r\n",
        "    * *turn off the fan*\r\n",
        "    * *turn the lamp off*\r\n",
        "    * *switch on the AC*\r\n",
        "3. 關閉測試窗格。\r\n",
        "    \r\n",
        "### 發佈模型和設定端點\r\n",
        "\r\n",
        "為了在用戶端應用程式中使用訓練模型，您必須將該模型作為端點發佈，用戶端應用程式可以向此端點傳送新表達；從此端點中可以預測意圖和實體。\r\n",
        "\r\n",
        "1. 在應用程式的 [語言理解] 頁面頂部，按一下 **[發佈]**。然後選取 **[生產位置]** 並按一下 **[完成]**。\r\n",
        "\r\n",
        "2. 發佈模型後，在應用程式的 [語言理解] 頁面頂部，按一下 **[管理]**。然後在 **[設定]**索引標籤上，請注意適用於您的應用程式的 **App ID**。複製此 App ID 並將其貼上到下方程式碼中，以取代 **YOUR_LU_APP_ID**。\r\n",
        "\r\n",
        "3. 在 **Azure 資源** 索引標籤上，請注意適用於您的預測資源的**主索引鍵**和**端點 URL**。將它們複製並貼上到下方程式碼中，取代 **YOUR_LU_KEY** 和 **YOUR_LU_ENDPOINT**。\r\n",
        "\r\n",
        "4. 透過按一下其 **[執行儲存格]** (&#9655;) 按鈕 (位於儲存格左側) 執行下方儲存格，出現提示時，輸入文字 **[turn the light on]**。這些文字由語言理解模型解譯並顯示適當的影像。\r\n",
        "\r\n",
        "### **(!)重點**： \r\n",
        "在您的視窗頂部尋找提示。您需要鍵入 *[turn the light on]* 並按 **[Enter]**。 \r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from python_code import luis\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "    # Set up API configuration\n",
        "    luis_app_id = 'YOUR_LU_APP_ID'\n",
        "    luis_key = 'YOUR_LU_KEY'\n",
        "    luis_endpoint = 'YOUR_LU_ENDPOINT'\n",
        "\n",
        "    # prompt for a command\n",
        "    command = input('Please enter a command: \\n')\n",
        "\n",
        "    # get the predicted intent and entity (code in python_code.home_auto.py)\n",
        "    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, command)\n",
        "\n",
        "    # display an appropriate image\n",
        "    img_name = action + '.jpg'\n",
        "    img = Image.open(os.path.join(\"data\", \"luis\" ,img_name))\n",
        "    plt.axis('off')\n",
        "    plt. imshow(img)\n",
        "except Exception as ex:\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1599696381331
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (!)簽入 \r\n",
        "您是否執行了上方的儲存格，然後在出現提示時鍵入片語 *[turn the light on]* ？該提示將出現在您的視窗頂部。  \r\n",
        "\r\n",
        "再次執行上方儲存格，嘗試下列片語：\r\n",
        "\r\n",
        "* *turn on the light*\r\n",
        "* *put the lamp off*\r\n",
        "* *switch the fan on*\r\n",
        "* *switch the light on*\r\n",
        "* *switch off the light*\r\n",
        "* *turn off the fan*\r\n",
        "* *switch the AC on*\r\n",
        "\r\n",
        "若您執行了上方儲存格並顯示了問號影像，您可能在建立實體、意圖或表達時使用了與指示稍有不同的文字或空格。\r\n",
        "\r\n",
        "> **備註**：若您對用於從語言理解應用程式中取出意圖和實體的程式碼感到好奇，請查看 **python_code** 資料夾中的 **luis.py** 檔案。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 新增語音控制\r\n",
        "\r\n",
        "到目前為止，我們已經知道了如何分析文字，但日益成熟的 AI 系統使得人類能夠透過語音辨識與軟體服務交談。為了對此提供支援，**語音**認知服務提供了一種將語音語言轉譯為文字的簡單方法。\r\n",
        "\r\n",
        "### 建立認知服務資源\r\n",
        "\r\n",
        "如果您還沒有該資源，請使用以下步驟在您的 Azure 訂用帳戶中建立一個**認知服務**資源：\r\n",
        "\r\n",
        "> **備註**：若您已經創建了認知服務資源，只需開啟其位於 Azure 入口網站中的 **[快速入門]** 頁面，將該資源的金鑰和位置複製到下方儲存格中。否則，可追隨下面的步驟來建立一個認知服務資源。\r\n",
        "\r\n",
        "1. 在其它瀏覽器索引標籤中，透過 [https://portal.azure.com](https://portal.azure.com) 開啟 Azure 入口網站，並用您的 Microsoft 帳戶登入。\r\n",
        "2. 按一下 **[&#65291; 建立資源]** 按鈕，搜尋認知服務**，並建立包含以下設定的**認知服務**資源：\r\n",
        "    - **訂用帳戶**： *您的 Azure 訂用帳戶*。\r\n",
        "    - **資源群組**： *選取或建立具有唯一名稱的資源群組*。\r\n",
        "    - **區域**： *選擇任一可用區域*：\r\n",
        "    - **名稱**： *輸入唯一名稱*。\r\n",
        "    - **定價層**：S0\r\n",
        "    - **選取此方塊時，即表示我認證此服務不用於或提供給美國的警察部門**：已選取。\r\n",
        "    - **我確認已閱讀通知並理解通知內容**：已選取。\r\n",
        "3. 等待部署完成。然後即可轉到您的認知服務資源，在 **[快速入門]** 頁面請注意金鑰和位置。您需要用金鑰和位置從用戶端應用程式連線到您的認知服務資源。\r\n",
        "\r\n",
        "### 獲取適用於認知服務資源的金鑰和位置\r\n",
        "\r\n",
        "若要使用您的認知服務資源，用戶端應用程式需要其驗證金鑰和位置：\r\n",
        "\r\n",
        "1. 在 Azure 入口網站中，您的認知服務資源之 **[金鑰和端點]** 頁面上，複製您的資源之**金鑰 1** 並將其貼上到下面的程式碼，取代 **YOUR_COG_KEY**。\r\n",
        "2. 複製您的資源之**位置**並將其貼上到下方程式碼中，取代 **YOUR_COG_LOCATION**。\r\n",
        ">**備註**：留在 **[金鑰和端點]** 頁面並從此頁面複製**位置** (範例：_westus_)。請 _勿_ 在 [位置] 欄位的文字之間增加空格。 \r\n",
        "3. 執行以下儲存格中的代碼。 "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_location = 'YOUR_COG_LOCATION'\n",
        "\n",
        "print('Ready to use cognitive services in {} using key {}'.format(cog_location, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1599696409914
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "現在執行以下儲存格，以便轉譯音訊檔案中的語音，並用作您語言理解應用程式的命令。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from python_code import luis\n",
        "from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n",
        "from playsound import playsound\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "try:   \n",
        "\n",
        "    # Get spoken command from audio file\n",
        "    file_name = 'light-on.wav'\n",
        "    audio_file = os.path.join('data', 'luis', file_name)\n",
        "\n",
        "    # Configure speech recognizer\n",
        "    speech_config = SpeechConfig(cog_key, cog_location)\n",
        "    audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n",
        "    speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n",
        "\n",
        "    # Use a one-time, synchronous call to transcribe the speech\n",
        "    speech = speech_recognizer.recognize_once()\n",
        "\n",
        "    # Get the predicted intent and entity (code in python_code.home_auto.py)\n",
        "    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, speech.text)\n",
        "\n",
        "    # Get the appropriate image\n",
        "    img_name = action + '.jpg'\n",
        "\n",
        "    # Display image \n",
        "    img = Image.open(os.path.join(\"data\", \"luis\" ,img_name))\n",
        "    plt.axis('off')\n",
        "    plt. imshow(img)\n",
        "    playsound(audio_file)\n",
        "\n",
        "except Exception as ex:\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1599696420498
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "嘗試修改上方儲存格以便使用 **light-off.wav** 音訊檔案。\r\n",
        "\r\n",
        "## 瞭解更多資訊\r\n",
        "\r\n",
        "了解更多關於語言理解的資訊詳見 [服務文件](https://docs.microsoft.com/azure/cognitive-services/luis/)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}